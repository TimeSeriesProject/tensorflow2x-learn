
## 优化器

![img.png](../imgs/op/优化器-1.png)

### 1. SGD(无momentum), 常用的梯度下降法
![img.png](../imgs/op/sgd.png)
```
w1.assign_sub(lr * grad[0])
b1.assign_sub(lr * grad[1])
```

### 2.SGDM
![img.png](../imgs/op/sgdm.png)


### 3.Adagrad， 在sgd基础上增加二阶动量
![img.png](../imgs/op/adagrad.png)


### 4.RMSProp优化器
![img.png](../imgs/op/rmsprop.png)

### 5.Adam, 同时结合SGDM一阶动量和RMSProp二阶动量
![img_1.png](../imgs/op/adam.png)
![img.png](../imgs/op/adam-2.png)
### 优化器训练时间对比

sgd: 6.829
sgdm: 9.661
adagrad: 7.80